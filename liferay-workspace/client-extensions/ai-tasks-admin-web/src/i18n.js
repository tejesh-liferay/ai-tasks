import { initReactI18next } from 'react-i18next';

import i18n from 'i18next';

const resources = {
  en: {
    translation: {
      'allowedFunctionNames-help': 'Enter the allowed function names.',
      'allowedFunctionNames-label': 'Allowed Function Names',
      'allowedFunctionNames-placeholder': '',
      'accessToken-help':
        'Enter the access token. To specify an access token using an environment variable, prefix the environment variable name with env:',
      'accessToken-label': 'Access Token',
      'accessToken-placeholder': 'Enter the access token',
      advancedÍ„: 'Advanced',
      anthropicChatModel: 'Anthropic Chat Model',
      'anthropic-chat-model-node-description':
        "The Anthropic Chat Model Node allows you to integrate Anthropic's large language models (LLMs) into your AI Task flows. This node enables you to leverage Anthropic's natural language processing capabilities for tasks such as conversation, text generation, summarization, and more.",
      'apiKey-help':
        'Enter the API key. To specify an API key using an environment variable, prefix the environment variable name with env:',
      'apiKey-label': 'API Key',
      'apiKey-placeholder': 'Enter the API key',
      'baseUrl-help':
        'Enter the base URL. To specify the value using an environment variable, prefix the environment variable name with env:',
      'baseUrl-label': 'Base URL',
      'baseUrl-placeholder': 'Enter the base URL',
      'beta-help': 'Enter the beta name to use.',
      'beta-label': 'Beta Name',
      'beta-placeholder': '',
      blueprint: 'Blueprint',
      'cacheSystemMessages-help': 'Cache system messages.',
      'cacheSystemMessages-label': 'Cache System Messages',
      'cacheTools-help': 'Cache tools.',
      'cacheTools-label': 'Cache Tools',
      'customHeaders-label': 'Custom Headers',
      'documentResultField-label': 'Document Result Field',
      'documentResultField-help':
        'The search engine document field to use for extracting the content from search results for grounding the LLM within a RAG pipeline.',
      'documentResultField-placeholder': '',
      'endpoint-help': '',
      'endpoint-label': 'Endpoint',
      'endpoint-placeholder': '',
      'frequencyPenalty-help': '',
      'frequencyPenalty-label': 'Presence Penalty',
      'frequencyPenalty-placeholder': '',
      geminiChatModel: 'Gemini Chat Model',
      'gemini-chat-model-node-description':
        "The Gemini Chat Model Node allows you to seamlessly integrate Google's powerful Gemini family of large language models (LLMs) into your AI Task flows. This node enables you to utilize Gemini's advanced natural language processing capabilities for a wide variety of tasks, including conversational AI, text generation, content summarization, question answering, and more.",
      general: 'General',
      googleImagen: 'Google Imagen',
      'google-imagen-node-description':
        "The Google Imagen Node allows you to harness the power of Google's Imagen text-to-image generation models within your AI Task flows. This node enables you to create stunning and high-quality images from textual descriptions, opening up a world of creative possibilities.",
      'guidanceScale-help': '',
      'guidanceScale-label': 'Guidance Scale',
      'guidanceScale-placeholder': '',
      'httpMethod-help': '',
      'httpMethod-label': 'HTTP Method',
      'httpMethod-placeholder': '',
      huggingFaceChatModel: 'Hugging Face Chat Model',
      'hugging-face-chat-model-node-description':
        "The Hugging Face Chat Model Node allows you to integrate a wide array of chat-based large language models (LLMs) from Hugging Face's vast model hub into your AI Task flows. This node empowers you to leverage the power of open-source and community-driven AI for conversational AI, text generation, content summarization, question answering, and more.",
      'input-and-output': 'Input & Output',
      'inputParameterName-help':
        "The name of the input parameter that hold the search query. Usually it's 'text'.",
      'inputParameterName-label': 'Input Parameter Name',
      'inputParameterName-placeholder': '',
      inputTrigger: 'Input Trigger',
      'language-help': 'Enter the language code.',
      'language-label': 'Language',
      'language-placeholder': '',
      'liferay-search-node-description':
        "The Liferay Search Node allows you to leverage Liferay's  search capabilities within an AI Task flow. This node executes a search query, against a pre-configured Liferay Search Blueprint if defined, and returns the results. You can use this node to ground LLM responses by outputting the results into the task context and inserting them as the context for an LLM node. This is useful for building Retrieval Augmented Generation (RAG) pipelines.",
      liferaySearch: 'Liferay Search',
      'location-help': '',
      'location-label': 'Location',
      'location-placeholder': '',
      'logitBias-label': 'Logit Bias',
      'logRequests-help': 'Log requests in the console.',
      'logRequests-label': 'Log Requests',
      'logResponses-help': 'Log responses in the console.',
      'logResponses-label': 'Log Responses',
      'maxRetries-help': 'Enter the maximum number of retries in case of API call failure.',
      'maxRetries-label': 'Max Retries',
      'maxRetries-placeholder': '',
      'maxNewTokens-help': 'Enter the maximum number of tokens in the output.',
      'maxNewTokens-label': 'Max New Tokens',
      'maxNewTokens-placeholder': '',
      'maxOutputTokens-help': 'Enter the maximum number of tokens in the output.',
      'maxOutputTokens-label': 'Max Output Tokens',
      'maxOutputTokens-placeholder': '',
      'maxTokens-help': 'Enter the maximum number of tokens in the output.',
      'maxTokens-label': 'Max Tokens',
      'maxTokens-placeholder': '',
      'memoryMaxMessages-label': 'Chat Memory Max Messages',
      'memoryMaxMessages-help':
        'Enter the number of messages to keep in the memory. The minimum is 2.',
      'memoryMaxMessages-placeholder': '',
      mistralAIChatModel: 'Mistral AI Chat Model',
      'mistralai-chat-model-node-description':
        "The Mistral AI Chat Model Node allows you to integrate Mistral AI's cutting-edge large language models (LLMs) into your AI Task workflows. This node gives you access to Mistral's natural language processing capabilities, enabling you to create sophisticated applications for conversational AI, text generation, content summarization, question answering, and a variety of other text-based tasks.",
      model: 'Model',
      'model-help': '',
      'model-label': 'Model',
      'model-placeholder': '',
      'modelId-help': 'Enter the ID of the model to use.',
      'modelId-label': 'Model ID',
      'modelId-placeholder': '',
      modelName: 'Model Name',
      'modelName-label': 'Model Name',
      'modelName-help': 'Enter the name of the model to use.',
      'modelName-placeholder': 'Enter the model name',
      'negativePrompt-help': "Define things you don't want to see in the picture.",
      'negativePrompt-label': 'Negative Prompt',
      'negativePrompt-placeholder': '',
      'numCtx-help':
        'Enter the size of the context window used to generate the next token. (Default: 2048).',
      'numCtx-label': 'Context Window Size',
      'numCtx-placeholder': '',

      'numPredict-help':
        'Enter the maximum number of tokens to predict when generating text. (Default: 128, -1 = infinite generation, -2 = fill context).',
      'numPredict-label': 'Number of Predictions',
      'numPredict-placeholder': '',
      ollamaChatModel: 'Ollama Chat Model',
      'ollama-chat-model-node-description':
        'The Ollama Chat Model Node allows you to seamlessly integrate locally-run large language models (LLMs) managed by Ollama into your AI Task flows. This node provides a powerful and flexible way to leverage the capabilities of open-source LLMs without relying on external APIs.',
      openAIChatModel: 'OpenAI Chat Model',
      'openai-chat-model-node-description':
        "The OpenAI Chat Model Node enables you to integrate OpenAI's powerful large language models (LLMs) into your AI Task flows. This node allows you to utilize OpenAI's advanced natural language processing capabilities for a wide range of tasks, including conversational AI, text generation, content summarization, question answering, and more.",
      'openai-image-model-node-description':
        "The OpenAI Image Model Node allows you to integrate OpenAI's powerful image generation models, such as DALL-E, into your AI Task flows. This node enables you to create unique and high-quality images from textual descriptions, expanding your creative possibilities and automating visual content creation.",
      openAIImageModel: 'OpenAI Image Model',
      output: 'Output',
      'outputParameterName-help': 'Enter the name of the output parameter.',
      'outputParameterName-label': 'Output Parameter Name',
      'outputParameterName-placeholder': '',
      'organizationId-help': '',
      'organizationId-label': 'Organization ID',
      'organizationId-placeholder': '',
      payload: 'Payload',
      'presencePenalty-help': '',
      'presencePenalty-label': 'Presence Penalty',
      'presencePenalty-placeholder': '',
      'project-help': 'The name of the project this configuration applies to.',
      'project-label': 'Project',
      'project-placeholder': '',
      prompt: 'Prompt',
      'promptTemplate-help': 'The prompt template to use.',
      'promptTemplate-label': 'Prompt Template',
      'promptTemplate-placeholder': 'Enter prompt template (e.g. {{input.text}})',
      'publisher-help': '',
      'publisher-label': 'Publisher',
      'publisher-placeholder': '',
      'quality-help': '',
      'quality-label': 'Quality',
      'quality-placeholder': '',
      'repeatPenalty-label': 'Repeat Penalty',
      'repeatPenalty-help':
        'Set how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1)',
      'repeatPenalty-placeholder': '',
      'requestBody-help': '',
      'requestBody-label': 'Request Body',
      'requestBody-placeholder': '',
      'requestHeaders-help': '',
      'requestHeaders-label': 'Request Headers',
      'requestHeaders-placeholder': '',
      responseFormat: 'Response Format',
      'responseFormat-label': 'Response Format',
      'responseFormat-help': 'Enter the format of the output.',
      'responseFormat-placeholder': '',
      'responseSchema-label': 'Response Schema',
      'responseSchema-help': 'Enter the response schema.',
      'responseSchema-placeholder': '',
      'returnFullText-help': 'Return full text.',
      'returnFullText-label': 'Return Full Text',
      'returnFullText-placeholder': '',
      'safePrompt-help': 'Use safe prompt.',
      'safePrompt-label': 'Safe Prompt',
      'safetySettings-help': '',
      'safetySettings-label': 'Safety Settings',
      'sampleImageSize-help': '',
      'sampleImageSize-label': 'Sample Image Size',
      'sampleImageSize-placeholder': '',
      'sampleImageStyle-help': '',
      'sampleImageStyle-label': 'Sample Image Style',
      'sampleImageStyle-placeholder': '',
      'seed-label': 'Seed',
      'seed-help':
        'Set the seed for the generation. Setting this to a specific value will make the model generate the same output for the same prompt.',
      'seed-placeholder': '',
      'size-help': '',
      'size-label': 'Size',
      'size-placeholder': '',
      'stop-help': 'Enter the stop tokens to use.',
      'stop-label': 'Stop',
      'stop-placeholder': '',
      'sxpBlueprintExternalReferenceCode-help':
        'The external reference of a blueprint performing the Liferay search.',
      'sxpBlueprintExternalReferenceCode-label': 'SXP Blueprint External Reference Code',
      'sxpBlueprintExternalReferenceCode-placeholder': '',
      'style-help': '',
      'style-label': 'Style',
      'style-placeholder': '',
      'systemMessage-help': 'Enter the instructions for the model to set up its behavior.',
      'systemMessage-label': 'System Message',
      'systemMessage-placeholder': 'Enter the system message',
      'taskContextOutputParameterName-help':
        'Enter the name of the task context output parameter. Use this if this node is not the final output node, but sharing output data with the next nodes in chain. Access the value in the next nodes using {{taskContext.YOUR_VARIABLE_NAME}}.',
      'taskContextOutputParameterName-label': 'Task Context Output Parameter Name',
      'taskContextOutputParameterName-placeholder': '',
      'temperature-help':
        'Controls the randomness of the output (0.0 - 1.0). Higher values result in more random output.',
      'temperature-label': 'Temperature',
      'timeout-help': 'Enter the timeout in seconds.',
      'timeout-label': 'Timeout',
      'timeout-placeholder': '',
      'toolProvider-help':
        'Enter the tool provider configuration. See documentation for more information.',
      'toolProvider-label': 'Tool Provider',
      tools: 'Tools',
      'tools-help': 'Enter the tools configuration. See documentation for more information.',
      'tools-label': 'Tools',
      'topK-help':
        'Enter the value for the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)',
      'topK-label': 'Top K',
      'topK-placeholder': '',
      'topP-help':
        'Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)',
      'topP-label': 'Top P',
      'topP-placeholder': '',
      url: 'URL',
      'url-help': '',
      'url-label': 'URL',
      'url-placeholder': '',
      'useCache-label': 'Use Cache',
      'useCache-help': 'Select whether to cache LLM responses.',
      'useChatMemory-label': 'Use Chat Memory',
      'useChatMemory-help': 'Select whether to memorize the conversation.',
      'useGoogleSearch-help': 'Ground response on Google search.',
      'useGoogleSearch-label': 'Use Google search',
      'user-help': '',
      'user-label': 'User',
      'user-placeholder': '',
      'vertexSearchDatastore-help': 'Enter the Vertex Search Data Store name.',
      'vertexSearchDatastore-label': 'Vertex Search Data Store',
      'vertexSearchDatastore-placeholder': '',
      'version-help': 'Enter the version name to use.',
      'version-label': 'Version Name',
      'version-placeholder': '',
      'waitForModel-help': 'Wait for the model.',
      'waitForModel-label': 'Wait for the Model',
      webhook: 'Webhook',
      'webhook-node-description':
        'The Webhook Node allows you to integrate with external services by sending HTTP requests to webhooks within your AI Task flows. This node provides a flexible way to trigger actions in external systems, retrieve data from third-party APIs, or orchestrate complex workflows.',
      'condition-type-label': 'Condition Type',
      equals: 'Equals',
      contains: 'Contains',
      in: 'In',
      exists: 'Exists',
      range: 'Range',
      not: 'Not',
      allConditions: 'All Conditions',
      anyConditions: 'Any Conditions',
      'condition-field-label': 'Condition Field',
      'condition-value-label': 'Condition Value',
      'exists-condition-description': 'This condition checks if the field exists.',
      'range-condition-label': 'Range Condition',
      'not-condition-label': 'Not Condition',
      'all-conditions-label': 'All Conditions',
      'any-conditions-label': 'Any Conditions',
      'remove-condition': 'Remove Condition',
      'add-condition': 'Add Condition',
      geminiStreamingChatModel: 'Gemini Streaming Chat Model',
      ollamaStreamingChatModel: 'Ollama Streaming Chat Model',
      openAIStreamingChatModel: 'OpenAI Streaming Chat Model',
      'ollama-streaming-chat-model-node-description':
        'The Ollama Streaming Chat Model Node allows you to seamlessly integrate locally-run large language models (LLMs) managed by Ollama into your AI Task flows, providing a streaming response. This node provides a powerful and flexible way to leverage the capabilities of open-source LLMs without relying on external APIs, and it provides the response in a streaming fashion.',
      'openai-streaming-chat-model-node-description':
        "The OpenAI Streaming Chat Model Node allows you to integrate OpenAI's powerful large language models (LLMs) into your AI Task flows, providing a streaming response.",
      'gemini-streaming-chat-model-node-description':
        "The Gemini Streaming Chat Model Node allows you to integrate Google's powerful Gemini family of large language models (LLMs) into your AI Task flows, providing a streaming response.",
    },
  },
};

i18n.use(initReactI18next).init({
  resources,
  lng: 'en',
  interpolation: {
    escapeValue: false,
  },
});

export default i18n;
